{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "main = '/Users/aklat/Downloads/Thsesis/Datasets/Moral Machine Data/SharedResponses.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ResponseID              ExtendedSessionID        UserID  \\\n",
      "0  2222bRQqBTZ6dLnPH    32757157_6999801415950060.0  6.999801e+15   \n",
      "1  2222sJk4DcoqXXi98        1043988516_3525281295.0  3.525281e+09   \n",
      "2  2223CNmvTr2Coj4wp  -1613944085_422160228641876.0  4.221602e+14   \n",
      "3  2223Xu54ufgjcyMR3   1425316635_327833569077076.0  3.278336e+14   \n",
      "4  2223jMWDEGNeszivb  -1683127088_785070916172117.0  7.850709e+14   \n",
      "\n",
      "   ScenarioOrder  Intervention  PedPed  Barrier  CrossingSignal  \\\n",
      "0              7             0       0        0               1   \n",
      "1              2             0       0        0               0   \n",
      "2             10             0       1        0               1   \n",
      "3             11             0       0        1               0   \n",
      "4              8             0       1        0               2   \n",
      "\n",
      "   NumberOfCharacters  DiffNumberOFCharacters  ...  LargeMan Criminal  \\\n",
      "0                 5.0                     0.0  ...       0.0      0.0   \n",
      "1                 1.0                     0.0  ...       0.0      0.0   \n",
      "2                 4.0                     0.0  ...       0.0      0.0   \n",
      "3                 5.0                     0.0  ...       0.0      0.0   \n",
      "4                 5.0                     2.0  ...       0.0      1.0   \n",
      "\n",
      "   MaleExecutive  FemaleExecutive  FemaleAthlete  MaleAthlete  FemaleDoctor  \\\n",
      "0            0.0              0.0            1.0          2.0           0.0   \n",
      "1            1.0              0.0            0.0          0.0           0.0   \n",
      "2            0.0              1.0            0.0          0.0           0.0   \n",
      "3            0.0              0.0            0.0          0.0           0.0   \n",
      "4            1.0              0.0            1.0          0.0           0.0   \n",
      "\n",
      "   MaleDoctor  Dog  Cat  \n",
      "0         0.0  0.0  0.0  \n",
      "1         0.0  0.0  0.0  \n",
      "2         0.0  0.0  0.0  \n",
      "3         0.0  0.0  0.0  \n",
      "4         0.0  1.0  0.0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load data with certain columns\n",
    "main = '/Users/aklat/Downloads/Thsesis/Datasets/Moral Machine Data/SharedResponses.csv'\n",
    "all_columns = pd.read_csv(main, nrows=0).columns.tolist()\n",
    "selected_columns = all_columns[:8] + all_columns[14:17] + all_columns[-20:]\n",
    "\n",
    "# create a new dataframe\n",
    "df_main = pd.DataFrame()\n",
    "\n",
    "# load data\n",
    "for chunk in pd.read_csv(main, usecols=selected_columns, chunksize=1000000, low_memory=False):\n",
    "    df_main = pd.concat([df_main, chunk], ignore_index=True)\n",
    "print(df_main.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70332355, 31)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.to_csv('main_feature_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'ResponseID', 'ExtendedSessionID', 'UserID',\n",
       "       'ScenarioOrder', 'Intervention', 'PedPed', 'Barrier', 'CrossingSignal',\n",
       "       'NumberOfCharacters', 'DiffNumberOFCharacters', 'Saved', 'Man', 'Woman',\n",
       "       'Pregnant', 'Stroller', 'OldMan', 'OldWoman', 'Boy', 'Girl', 'Homeless',\n",
       "       'LargeWoman', 'LargeMan', 'Criminal', 'MaleExecutive',\n",
       "       'FemaleExecutive', 'FemaleAthlete', 'MaleAthlete', 'FemaleDoctor',\n",
       "       'MaleDoctor', 'Dog', 'Cat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "f1_path = '/Users/aklat/Downloads/Thsesis/Datasets/Moral Machine Data/main_feature_1.csv'\n",
    "f1=pd.read_csv(f1_path,nrows=190)\n",
    "f1.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(463675, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/Users/aklat/Downloads/Thsesis/Datasets/Moral Machine Data/SharedResponsesSurvey.csv'\n",
    "\n",
    "# extract 'UserID'\n",
    "df = pd.read_csv(file_path, usecols=[\"UserID\"])\n",
    "\n",
    "# remove userid's duplicates\n",
    "df_unique = df.drop_duplicates()\n",
    "print(df_unique.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x5/b6qvk1zx3dg9d_8_8zt4hf3m0000gn/T/ipykernel_23210/3050306158.py:13: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(f1_path, chunksize=chunksize):\n",
      "/var/folders/x5/b6qvk1zx3dg9d_8_8zt4hf3m0000gn/T/ipykernel_23210/3050306158.py:13: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(f1_path, chunksize=chunksize):\n",
      "/var/folders/x5/b6qvk1zx3dg9d_8_8zt4hf3m0000gn/T/ipykernel_23210/3050306158.py:13: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(f1_path, chunksize=chunksize):\n",
      "/var/folders/x5/b6qvk1zx3dg9d_8_8zt4hf3m0000gn/T/ipykernel_23210/3050306158.py:13: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(f1_path, chunksize=chunksize):\n",
      "/var/folders/x5/b6qvk1zx3dg9d_8_8zt4hf3m0000gn/T/ipykernel_23210/3050306158.py:13: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(f1_path, chunksize=chunksize):\n",
      "/var/folders/x5/b6qvk1zx3dg9d_8_8zt4hf3m0000gn/T/ipykernel_23210/3050306158.py:13: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(f1_path, chunksize=chunksize):\n",
      "/var/folders/x5/b6qvk1zx3dg9d_8_8zt4hf3m0000gn/T/ipykernel_23210/3050306158.py:13: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(f1_path, chunksize=chunksize):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13127377, 32)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "f1_path = '/Users/aklat/Downloads/Thsesis/Datasets/Moral Machine Data/main_feature_1.csv'\n",
    "\n",
    "merged_chunks = []\n",
    "\n",
    "chunksize = 100000\n",
    "\n",
    "# merge 32 columns' main data with user data through 'UserID'\n",
    "for chunk in pd.read_csv(f1_path, chunksize=chunksize):\n",
    "    merged_chunk = pd.merge(df_unique, chunk, on='UserID', how='inner')\n",
    "    merged_chunks.append(merged_chunk)\n",
    "\n",
    "final_merged_df = pd.concat(merged_chunks, ignore_index=True)\n",
    "\n",
    "print(final_merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_df.to_csv('main_link_user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total_rows  unique_responseids\n",
      "UserID                                      \n",
      "2.173558e+09          22                  11\n",
      "2.271406e+09         202                 104\n",
      "2.371697e+09         129                  68\n",
      "2.372729e+09         575                 299\n",
      "2.423489e+09          24                  12\n",
      "...                  ...                 ...\n",
      "9.999948e+15          22                  11\n",
      "9.999965e+15          24                  12\n",
      "9.999990e+15          22                  12\n",
      "9.999993e+15          99                  52\n",
      "9.999993e+15          23                  12\n",
      "\n",
      "[339406 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# check structure of current data, filter those whose number of rows != 26 & number of unique ResponseID !=13\n",
    "df=final_merged_df\n",
    "\n",
    "grouped_df = df.groupby('UserID').agg(total_rows=('UserID', 'size'), unique_responseids=('ResponseID', 'nunique'))\n",
    "result_df = grouped_df[(grouped_df['total_rows'] != 26) & (grouped_df['unique_responseids'] != 13)]\n",
    "\n",
    "print(result_df)\n",
    "\n",
    "# 340k out of 460k users' data is not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_rows_bin\n",
      "(0, 26]           238509\n",
      "(26, 52]           75089\n",
      "(52, 78]           16222\n",
      "(78, 104]           5108\n",
      "(104, 130]          2035\n",
      "                   ...  \n",
      "(25896, 25922]         1\n",
      "(50336, 50362]         1\n",
      "(52728, 52754]         1\n",
      "(53872, 53898]         1\n",
      "(61802, 61828]         1\n",
      "Length: 61, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x5/b6qvk1zx3dg9d_8_8zt4hf3m0000gn/T/ipykernel_23210/79593751.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df['total_rows_bin'] = pd.cut(result_df['total_rows'], bins)\n",
      "/var/folders/x5/b6qvk1zx3dg9d_8_8zt4hf3m0000gn/T/ipykernel_23210/79593751.py:7: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  grouped = result_df.groupby('total_rows_bin').size()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# analyze distribution of users of which number of data is abnormal\n",
    "bins = np.arange(0, result_df['total_rows'].max() + 26, 26)\n",
    "result_df['total_rows_bin'] = pd.cut(result_df['total_rows'], bins)\n",
    "grouped = result_df.groupby('total_rows_bin').size()\n",
    "grouped = grouped[grouped != 0]\n",
    "print(grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy of filtering data:\n",
    "\n",
    "Step 1: Group by UserID and calculate the total data volume and unique ResponseID number for each user. If total=26 and unique=13, keep all data of these users.\n",
    "Step 2: If total<26, filter out all rows where ResponseID appears twice and the Saved column is 0 and 1 respectively, and keep this part of the data.\n",
    "Step 3: If total>26, randomly select 13 ResponseIDs, and these ResponseIDs appear twice and the Saved column is 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               UserID  Unnamed: 0         ResponseID  \\\n",
      "0        7.981091e+15       94911  2A8SAxAa6dLGK4bGm   \n",
      "1        9.964404e+15       51566  26Q8hd6RncCNv6gut   \n",
      "2        9.964404e+15       99848  2AZSA9oesRcN5utcx   \n",
      "3        5.209783e+15       35711  254hWwD8y4bcCtxp8   \n",
      "4        7.888200e+14       43929  25mYXXQZzcAg2sG5E   \n",
      "...               ...         ...                ...   \n",
      "9704245  9.999993e+15    59198029  vv48Mu99wjf7tbmES   \n",
      "9704246  9.999993e+15    61751683  zuC33ubGwCsfsCyCq   \n",
      "9704247  9.999993e+15    62426718  ndyokXboZiRhJiSnZ   \n",
      "9704248  9.999993e+15    67721180  vv48Mu99wjf7tbmES   \n",
      "9704249  9.999993e+15    70264960  zuC33ubGwCsfsCyCq   \n",
      "\n",
      "                      ExtendedSessionID  ScenarioOrder  Intervention  PedPed  \\\n",
      "0        -2119938054_7981090920342526.0             11             0       1   \n",
      "1        -2119627065_9964404106140136.0              6             0       1   \n",
      "2        -2119627065_9964404106140136.0             10             0       0   \n",
      "3        -2117293878_5209782729596508.0              6             0       0   \n",
      "4         -2112010153_788820015732199.0              6             0       0   \n",
      "...                                 ...            ...           ...     ...   \n",
      "9704245   1229057596_9999992505199810.0              9             0       1   \n",
      "9704246   1229057596_9999992505199810.0              1             0       0   \n",
      "9704247  -1262399492_9999992505199810.0              6             1       0   \n",
      "9704248   1229057596_9999992505199810.0              9             1       1   \n",
      "9704249   1229057596_9999992505199810.0              1             1       0   \n",
      "\n",
      "         Barrier  CrossingSignal  NumberOfCharacters  ...  LargeMan  Criminal  \\\n",
      "0              0               2                 3.0  ...       0.0       0.0   \n",
      "1              0               0                 4.0  ...       1.0       0.0   \n",
      "2              0               0                 3.0  ...       0.0       1.0   \n",
      "3              1               0                 1.0  ...       0.0       0.0   \n",
      "4              0               0                 1.0  ...       0.0       0.0   \n",
      "...          ...             ...                 ...  ...       ...       ...   \n",
      "9704245        0               2                 2.0  ...       0.0       0.0   \n",
      "9704246        0               0                 3.0  ...       0.0       0.0   \n",
      "9704247        1               0                 2.0  ...       0.0       0.0   \n",
      "9704248        0               1                 2.0  ...       0.0       0.0   \n",
      "9704249        1               0                 3.0  ...       0.0       0.0   \n",
      "\n",
      "        MaleExecutive  FemaleExecutive  FemaleAthlete  MaleAthlete  \\\n",
      "0                 0.0              0.0            0.0          0.0   \n",
      "1                 0.0              0.0            1.0          0.0   \n",
      "2                 0.0              0.0            0.0          1.0   \n",
      "3                 0.0              0.0            0.0          0.0   \n",
      "4                 0.0              0.0            0.0          0.0   \n",
      "...               ...              ...            ...          ...   \n",
      "9704245           0.0              0.0            0.0          0.0   \n",
      "9704246           0.0              0.0            0.0          0.0   \n",
      "9704247           0.0              0.0            0.0          1.0   \n",
      "9704248           0.0              0.0            0.0          1.0   \n",
      "9704249           0.0              0.0            0.0          0.0   \n",
      "\n",
      "         FemaleDoctor  MaleDoctor  Dog  Cat  \n",
      "0                 0.0         0.0  0.0  0.0  \n",
      "1                 0.0         1.0  0.0  0.0  \n",
      "2                 0.0         0.0  0.0  0.0  \n",
      "3                 0.0         0.0  0.0  0.0  \n",
      "4                 0.0         0.0  0.0  0.0  \n",
      "...               ...         ...  ...  ...  \n",
      "9704245           0.0         0.0  0.0  0.0  \n",
      "9704246           0.0         0.0  0.0  0.0  \n",
      "9704247           0.0         0.0  0.0  0.0  \n",
      "9704248           0.0         0.0  0.0  0.0  \n",
      "9704249           0.0         0.0  0.0  0.0  \n",
      "\n",
      "[9704250 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# group by UserID and calculate total rows per user\n",
    "grouped_df = final_merged_df.groupby('UserID').agg(\n",
    "    total_rows=('UserID', 'size'),  \n",
    "    unique_responseids=('ResponseID', 'nunique')  \n",
    ").reset_index()\n",
    "\n",
    "# Step 1: keep data that total = 26 & unique = 13 \n",
    "condition_1 = grouped_df[(grouped_df['total_rows'] == 26) & (grouped_df['unique_responseids'] == 13)]\n",
    "filtered_df_1 = final_merged_df[final_merged_df['UserID'].isin(condition_1['UserID'])]\n",
    "\n",
    "# Step 2: for users whose total < 26，make sure responseid is paired\n",
    "condition_2 = grouped_df[grouped_df['total_rows'] < 26]\n",
    "\n",
    "# total < 26\n",
    "filtered_df_2 = final_merged_df[final_merged_df['UserID'].isin(condition_2['UserID'])]\n",
    "\n",
    "# filter ResponseID = 2 & Saved contains 0 and 1\n",
    "def filter_responseid_group(group):\n",
    "    valid_responses = group.groupby('ResponseID').filter(lambda x: (len(x) == 2) and (x['Saved'].nunique() == 2))\n",
    "    return valid_responses\n",
    "\n",
    "filtered_df_2 = filtered_df_2.groupby('UserID').apply(filter_responseid_group).reset_index(drop=True)\n",
    "\n",
    "# Step 3: for users whose total > 26 的用户，pick 13 valid ResponseID randomaly\n",
    "condition_3 = grouped_df[grouped_df['total_rows'] > 26]\n",
    "\n",
    "# total > 26\n",
    "filtered_df_3 = final_merged_df[final_merged_df['UserID'].isin(condition_3['UserID'])]\n",
    "\n",
    "def random_sample(group):\n",
    "    valid_responses = group.groupby('ResponseID').filter(lambda x: (len(x) == 2) and (x['Saved'].nunique() == 2))\n",
    "    \n",
    "    if len(valid_responses['ResponseID'].unique()) > 13:\n",
    "        selected_responseids = np.random.choice(valid_responses['ResponseID'].unique(), size=13, replace=False)\n",
    "        \n",
    "        sample_responses = valid_responses[valid_responses['ResponseID'].isin(selected_responseids)]\n",
    "        return sample_responses\n",
    "    else:\n",
    "        return valid_responses\n",
    "\n",
    "filtered_df_3 = filtered_df_3.groupby('UserID').apply(random_sample).reset_index(drop=True)\n",
    "\n",
    "# concat\n",
    "final_filtered_df = pd.concat([filtered_df_1, filtered_df_2, filtered_df_3], ignore_index=True)\n",
    "\n",
    "print(final_filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_filtered_df.to_csv('clean_predata_1020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# transform float in column[11:] to int\n",
    "cols_to_convert = final_filtered_df.columns[11:]  \n",
    "\n",
    "for col in cols_to_convert:\n",
    "    final_filtered_df[col] = final_filtered_df[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_convert = final_filtered_df.columns[9:10]\n",
    "final_filtered_df[col] = final_filtered_df[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column; Saved\n",
      "Saved\n",
      "0    4852125\n",
      "1    4852125\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; Man\n",
      "Man\n",
      "0    7299193\n",
      "1    1918297\n",
      "2     410689\n",
      "3      68358\n",
      "4       7351\n",
      "5        362\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; Woman\n",
      "Woman\n",
      "0    7296524\n",
      "1    1921144\n",
      "2     410432\n",
      "3      68411\n",
      "4       7375\n",
      "5        364\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; Pregnant\n",
      "Pregnant\n",
      "0    9141417\n",
      "1     519705\n",
      "2      41258\n",
      "3       1825\n",
      "4         43\n",
      "5          2\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; Stroller\n",
      "Stroller\n",
      "0    9148101\n",
      "1     513117\n",
      "2      41180\n",
      "3       1802\n",
      "4         49\n",
      "5          1\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; OldMan\n",
      "OldMan\n",
      "0    8249280\n",
      "1    1090017\n",
      "2     274991\n",
      "3      74345\n",
      "4      14342\n",
      "5       1275\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; OldWoman\n",
      "OldWoman\n",
      "0    8249616\n",
      "1    1090784\n",
      "2     273766\n",
      "3      74475\n",
      "4      14382\n",
      "5       1227\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; Boy\n",
      "Boy\n",
      "0    8380495\n",
      "1    1079994\n",
      "2     206599\n",
      "3      33344\n",
      "4       3649\n",
      "5        169\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; Girl\n",
      "Girl\n",
      "0    8380923\n",
      "1    1078831\n",
      "2     207613\n",
      "3      33211\n",
      "4       3488\n",
      "5        184\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; Homeless\n",
      "Homeless\n",
      "0    9056893\n",
      "1     574183\n",
      "2      61916\n",
      "3       8749\n",
      "4       2089\n",
      "5        420\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; LargeWoman\n",
      "LargeWoman\n",
      "0    8383074\n",
      "1    1078368\n",
      "2     205884\n",
      "3      33190\n",
      "4       3560\n",
      "5        174\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; LargeMan\n",
      "LargeMan\n",
      "0    8383894\n",
      "1    1077627\n",
      "2     205919\n",
      "3      33013\n",
      "4       3639\n",
      "5        158\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; Criminal\n",
      "Criminal\n",
      "0    9148179\n",
      "1     513589\n",
      "2      40622\n",
      "3       1824\n",
      "4         35\n",
      "5          1\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; MaleExecutive\n",
      "MaleExecutive\n",
      "0    8813816\n",
      "1     788999\n",
      "2      93091\n",
      "3       7928\n",
      "4        410\n",
      "5          6\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; FemaleExecutive\n",
      "FemaleExecutive\n",
      "0    8812057\n",
      "1     790406\n",
      "2      93441\n",
      "3       7923\n",
      "4        417\n",
      "5          6\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; FemaleAthlete\n",
      "FemaleAthlete\n",
      "0    8250991\n",
      "1    1089651\n",
      "2     274109\n",
      "3      73708\n",
      "4      14487\n",
      "5       1304\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; MaleAthlete\n",
      "MaleAthlete\n",
      "0    8250419\n",
      "1    1089847\n",
      "2     274215\n",
      "3      74106\n",
      "4      14344\n",
      "5       1319\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; FemaleDoctor\n",
      "FemaleDoctor\n",
      "0    8838610\n",
      "1     766834\n",
      "2      90737\n",
      "3       7658\n",
      "4        399\n",
      "5         12\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; MaleDoctor\n",
      "MaleDoctor\n",
      "0    8837819\n",
      "1     766886\n",
      "2      91397\n",
      "3       7745\n",
      "4        392\n",
      "5         11\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; Dog\n",
      "Dog\n",
      "0    8591132\n",
      "1     695449\n",
      "2     255590\n",
      "3     119264\n",
      "4      37414\n",
      "5       5401\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "column; Cat\n",
      "Cat\n",
      "0    8592119\n",
      "1     694965\n",
      "2     255659\n",
      "3     118666\n",
      "4      37484\n",
      "5       5357\n",
      "Name: count, dtype: int64\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "df = final_filtered_df\n",
    "cols = df.columns[11:]\n",
    "\n",
    "# calculate unique values in each column\n",
    "for col in cols:\n",
    "    value_counts = df[col].value_counts()\n",
    "    print(f\"column; {col}\")\n",
    "    print(value_counts)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_filtered_df.to_csv('clean_predata_1020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UserID', 'Unnamed: 0', 'ResponseID', 'ExtendedSessionID',\n",
       "       'ScenarioOrder', 'Intervention', 'PedPed', 'Barrier', 'CrossingSignal',\n",
       "       'NumberOfCharacters', 'DiffNumberOFCharacters', 'Saved', 'Man', 'Woman',\n",
       "       'Pregnant', 'Stroller', 'OldMan', 'OldWoman', 'Boy', 'Girl', 'Homeless',\n",
       "       'LargeWoman', 'LargeMan', 'Criminal', 'MaleExecutive',\n",
       "       'FemaleExecutive', 'FemaleAthlete', 'MaleAthlete', 'FemaleDoctor',\n",
       "       'MaleDoctor', 'Dog', 'Cat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_filtered_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization\n",
    "gender_vec = {'unknown':0,'male':1,'female':2}\n",
    "age_vec = {'unknown':0,'baby':1,'young':2,'adult':3,'old':4}\n",
    "bodt_type_vec={'normal':0,'large':1}\n",
    "profession_vec = {'crinimal':-2,'homeless':-1,'unknown':0,'AthDocExe':1}\n",
    "pet_vec = {'human':0,'animal':1}\n",
    "pregnant = {'normal':0,'pregnant':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         UserID         ResponseID               ExtendedSessionID  \\\n",
      "0  7.981091e+15  2A8SAxAa6dLGK4bGm  -2119938054_7981090920342526.0   \n",
      "1  9.964404e+15  26Q8hd6RncCNv6gut  -2119627065_9964404106140136.0   \n",
      "2  9.964404e+15  2AZSA9oesRcN5utcx  -2119627065_9964404106140136.0   \n",
      "3  5.209783e+15  254hWwD8y4bcCtxp8  -2117293878_5209782729596508.0   \n",
      "4  7.888200e+14  25mYXXQZzcAg2sG5E   -2112010153_788820015732199.0   \n",
      "\n",
      "   ScenarioOrder  NumberOfCharacters  DiffNumberOFCharacters  Saved  \\\n",
      "0             11                 3.0                     0.0      0   \n",
      "1              6                 4.0                     1.0      0   \n",
      "2             10                 3.0                     2.0      0   \n",
      "3              6                 1.0                     0.0      1   \n",
      "4              6                 1.0                     2.0      1   \n",
      "\n",
      "  gender_vec age_vec body_type_vec profession_vec pet_vec pregnant_vec  \\\n",
      "0        [5]     [8]           [0]            [0]     [0]          [0]   \n",
      "1        [4]     [9]           [1]            [1]     [0]          [0]   \n",
      "2        [2]     [6]           [0]           [-1]     [0]          [0]   \n",
      "3        [1]     [3]           [0]            [0]     [0]          [0]   \n",
      "4        [2]     [3]           [0]            [0]     [0]          [1]   \n",
      "\n",
      "   Intervention_vec  Barrier_vec  CrossingSignal_vec  \n",
      "0                 0            0                  -1  \n",
      "1                 0            0                   0  \n",
      "2                 0            0                   0  \n",
      "3                 0            1                   0  \n",
      "4                 0            0                   0  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# vectorize\n",
    "def vectorize_character(character_type, count):\n",
    "    if count == 0:\n",
    "        return None  \n",
    "    \n",
    "    if character_type == 'Man':\n",
    "        gender_vec = [1]\n",
    "        age_vec = [3]\n",
    "        body_type_vec = [0]\n",
    "        profession_vec = [0]\n",
    "        pet_vec = [0]\n",
    "        pregnant_vec = [0]\n",
    "    elif character_type == 'Woman':\n",
    "        gender_vec = [2]\n",
    "        age_vec = [3]\n",
    "        body_type_vec = [0]\n",
    "        profession_vec = [0]\n",
    "        pet_vec = [0]\n",
    "        pregnant_vec = [0]\n",
    "    elif character_type == 'Pregnant':\n",
    "        gender_vec = [2]\n",
    "        age_vec = [3]\n",
    "        body_type_vec = [0]\n",
    "        profession_vec = [0]\n",
    "        pet_vec = [0]\n",
    "        pregnant_vec = [1]\n",
    "    elif character_type == 'Stroller':\n",
    "        gender_vec = [0]\n",
    "        age_vec = [1]\n",
    "        body_type_vec = [0]\n",
    "        profession_vec = [0]\n",
    "        pet_vec = [0]\n",
    "        pregnant_vec = [0]\n",
    "    elif character_type == 'Boy':\n",
    "        gender_vec = [1]\n",
    "        age_vec = [2]\n",
    "        body_type_vec = [0]\n",
    "        profession_vec = [0]\n",
    "        pet_vec = [0]\n",
    "        pregnant_vec = [0]\n",
    "    elif character_type == 'Girl':\n",
    "        gender_vec = [2]\n",
    "        age_vec = [2]\n",
    "        body_type_vec = [0]\n",
    "        profession_vec = [0]\n",
    "        pet_vec = [0]\n",
    "        pregnant_vec = [0]\n",
    "    elif character_type == 'OldMan':\n",
    "        gender_vec = [1]\n",
    "        age_vec = [4]\n",
    "        body_type_vec = [0]\n",
    "        profession_vec = [0]\n",
    "        pet_vec = [0]\n",
    "        pregnant_vec = [0]\n",
    "    elif character_type == 'OldWoman':\n",
    "        gender_vec = [2]\n",
    "        age_vec = [4]\n",
    "        body_type_vec = [0]\n",
    "        profession_vec = [0]\n",
    "        pet_vec = [0]\n",
    "        pregnant_vec = [0]\n",
    "    elif character_type == 'Homeless':\n",
    "        gender_vec = [0]\n",
    "        age_vec = [0]\n",
    "        body_type_vec = [0]\n",
    "        profession_vec = [-1]\n",
    "        pet_vec = [0]\n",
    "        pregnant_vec = [0]\n",
    "    elif character_type == 'Criminal':\n",
    "        gender_vec = [0]\n",
    "        age_vec = [0]\n",
    "        body_type_vec = [0]\n",
    "        profession_vec = [-2]\n",
    "        pet_vec = [0]\n",
    "        pregnant_vec = [0]\n",
    "    elif character_type == 'LargeWoman':\n",
    "        gender_vec = [2]\n",
    "        age_vec = [3]\n",
    "        body_type_vec = [1]\n",
    "        profession_vec = [0]\n",
    "        pet_vec = [0]\n",
    "        pregnant_vec = [0]\n",
    "    elif character_type == 'LargeMan':\n",
    "        gender_vec = [1]\n",
    "        age_vec = [3]\n",
    "        body_type_vec = [1]\n",
    "        profession_vec = [0]\n",
    "        pet_vec = [0]\n",
    "        pregnant_vec = [0]\n",
    "    elif character_type == 'MaleExecutive' or character_type == 'MaleAthlete' or character_type == 'MaleDoctor':\n",
    "        gender_vec = [1]\n",
    "        age_vec = [3]\n",
    "        body_type_vec = [0]\n",
    "        profession_vec = [1]\n",
    "        pet_vec = [0]\n",
    "        pregnant_vec = [0]\n",
    "    elif character_type == 'FemaleDoctor' or character_type == 'FemaleExecutive' or character_type == 'FemaleAthlete':\n",
    "        gender_vec = [2]\n",
    "        age_vec = [3]\n",
    "        body_type_vec = [0]\n",
    "        profession_vec = [1]\n",
    "        pet_vec = [0]\n",
    "        pregnant_vec = [0]\n",
    "    elif character_type == 'Dog':\n",
    "        gender_vec = [0]\n",
    "        age_vec = [0]\n",
    "        body_type_vec = [0]\n",
    "        profession_vec = [0]\n",
    "        pet_vec = [1]\n",
    "        pregnant_vec = [0]\n",
    "    elif character_type == 'Cat':\n",
    "        gender_vec = [0]\n",
    "        age_vec = [0]\n",
    "        body_type_vec = [0]\n",
    "        profession_vec = [0]\n",
    "        pet_vec = [1]\n",
    "        pregnant_vec = [0]\n",
    "    \n",
    "    return {\n",
    "        \"gender_vec\": np.tile(gender_vec, (count, 1)).sum(axis=0),\n",
    "        \"age_vec\": np.tile(age_vec, (count, 1)).sum(axis=0),\n",
    "        \"body_type_vec\": np.tile(body_type_vec, (count, 1)).sum(axis=0),\n",
    "        \"profession_vec\": np.tile(profession_vec, (count, 1)).sum(axis=0),\n",
    "        \"pet_vec\": np.tile(pet_vec, (count, 1)).sum(axis=0),\n",
    "        \"pregnant_vec\": np.tile(pregnant_vec, (count, 1)).sum(axis=0)\n",
    "    }\n",
    "\n",
    "# add ntervention、Barrier、CrossingSignal\n",
    "def vectorize_scene_features(row, number_of_characters):\n",
    "    cross_signal = row['CrossingSignal']\n",
    "    if cross_signal == 2:\n",
    "        cross_signal = -1\n",
    "    \n",
    "    scene_vector = np.tile([row['Intervention'], row['Barrier'], cross_signal], number_of_characters)\n",
    "    return {\n",
    "        \"Intervention_vec\": scene_vector[0],\n",
    "        \"Barrier_vec\": scene_vector[1],\n",
    "        \"CrossingSignal_vec\": scene_vector[2]\n",
    "    }\n",
    "\n",
    "# process each row\n",
    "def vectorize_row(row):\n",
    "    total_vector = {\n",
    "        \"gender_vec\": 0,\n",
    "        \"age_vec\": 0,\n",
    "        \"body_type_vec\": 0,\n",
    "        \"profession_vec\": 0,\n",
    "        \"pet_vec\": 0,\n",
    "        \"pregnant_vec\": 0\n",
    "    }\n",
    "    \n",
    "    for character_type in ['Man', 'Woman', 'Pregnant', 'Stroller', 'OldMan', 'OldWoman', 'Boy', 'Girl', 'Homeless',\n",
    "                           'LargeWoman', 'LargeMan', 'Criminal', 'MaleExecutive', 'FemaleExecutive', 'FemaleAthlete',\n",
    "                           'MaleAthlete', 'FemaleDoctor', 'MaleDoctor', 'Dog', 'Cat']:\n",
    "        count = row[character_type]\n",
    "        char_vecs = vectorize_character(character_type, count)\n",
    "        if char_vecs:\n",
    "            for key in total_vector:\n",
    "                total_vector[key] += char_vecs[key]\n",
    "    \n",
    "    scene_vecs = vectorize_scene_features(row, int(row['NumberOfCharacters']))\n",
    "    \n",
    "    return {**total_vector, **scene_vecs}\n",
    "\n",
    "# keep part of orignial columns\n",
    "def create_new_dataframe(df):\n",
    "    new_rows = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        vectorized_data = vectorize_row(row)\n",
    "        new_row = {\n",
    "            \"UserID\": row[\"UserID\"],\n",
    "            \"ResponseID\": row[\"ResponseID\"],\n",
    "            \"ExtendedSessionID\": row[\"ExtendedSessionID\"],\n",
    "            \"ScenarioOrder\": row[\"ScenarioOrder\"],\n",
    "            \"NumberOfCharacters\": row[\"NumberOfCharacters\"],\n",
    "            \"DiffNumberOFCharacters\": row[\"DiffNumberOFCharacters\"],\n",
    "            \"Saved\": row[\"Saved\"]\n",
    "        }\n",
    "        new_row.update(vectorized_data)\n",
    "        new_rows.append(new_row)\n",
    "    \n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "\n",
    "df = final_filtered_df\n",
    "vectorized_df = create_new_dataframe(df)\n",
    "\n",
    "print(vectorized_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9704250, 16)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply number of characters to signal/intervention/barrier\n",
    "df2 = vectorized_df\n",
    "df2['CrossingSignal_vec']=df2['CrossingSignal_vec']*df2['NumberOfCharacters']\n",
    "df2['Intervention_vec']=df2['Intervention_vec']*df2['NumberOfCharacters']\n",
    "df2['Barrier_vec']=df2['Barrier_vec']*df2['NumberOfCharacters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UserID', 'ResponseID', 'ExtendedSessionID', 'ScenarioOrder',\n",
       "       'NumberOfCharacters', 'DiffNumberOFCharacters', 'Saved', 'gender_vec',\n",
       "       'age_vec', 'body_type_vec', 'profession_vec', 'pet_vec', 'pregnant_vec',\n",
       "       'Intervention_vec', 'Barrier_vec', 'CrossingSignal_vec'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in vecctorized_df, these columns' data are like; [number], so remove []\n",
    "cols_to_convert = ['gender_vec', 'age_vec', 'body_type_vec', 'profession_vec', 'pet_vec', 'pregnant_vec']\n",
    "for col in cols_to_convert:\n",
    "    df2[col] = df2[col].apply(lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv ('vectorized data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    4852125\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # calculate each responseid's number of rows\n",
    "# responseid_count = df2.groupby('ResponseID').size()\n",
    "\n",
    "# # print distribution\n",
    "# distribution = responseid_count.value_counts().sort_index()\n",
    "\n",
    "# print(distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction through RankNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = vectorized_df\n",
    "\n",
    "grouped = df.groupby('ResponseID')\n",
    "\n",
    "# create RankNet's paired input\n",
    "ranknet_pairs = []\n",
    "ranknet_labels = []\n",
    "\n",
    "for _, group in grouped:\n",
    "    if len(group) == 2:  \n",
    "        row1, row2 = group.iloc[0], group.iloc[1]\n",
    "        \n",
    "        # create feature pair\n",
    "        pair_features_1 = [\n",
    "            row1['gender_vec'], row1['age_vec'], row1['body_type_vec'], \n",
    "            row1['profession_vec'], row1['pet_vec'], row1['pregnant_vec'],\n",
    "            row1['Intervention_vec'], row1['Barrier_vec'], row1['CrossingSignal_vec']\n",
    "        ]\n",
    "        \n",
    "        pair_features_2 = [\n",
    "            row2['gender_vec'], row2['age_vec'], row2['body_type_vec'], \n",
    "            row2['profession_vec'], row2['pet_vec'], row2['pregnant_vec'],\n",
    "            row2['Intervention_vec'], row2['Barrier_vec'], row2['CrossingSignal_vec']\n",
    "        ]\n",
    "        \n",
    "        ranknet_pairs.append((pair_features_1, pair_features_2))\n",
    "        \n",
    "        # create label,saved==1 > saved==0\n",
    "        if row1['Saved'] > row2['Saved']:\n",
    "            ranknet_labels.append(1)  \n",
    "        else:\n",
    "            ranknet_labels.append(0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = ranknet_pairs  \n",
    "y = ranknet_labels  \n",
    "\n",
    "# train_test_validation_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class RankNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(RankNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build with acutal data\n",
    "input_dim = 9  # 9 vectorized features\n",
    "model = RankNet(input_dim)\n",
    "\n",
    "# loss & optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.957237720489502, Validation Loss: 0.5515197398303426\n",
      "Epoch 2, Training Loss: 0.8884412050247192, Validation Loss: 0.5508653697524813\n",
      "Epoch 3, Training Loss: 0.9828553199768066, Validation Loss: 0.550897734190582\n",
      "Epoch 4, Training Loss: 0.9651614427566528, Validation Loss: 0.5506160302084422\n",
      "Epoch 5, Training Loss: 0.9227561354637146, Validation Loss: 0.5504825637632111\n",
      "Epoch 6, Training Loss: 0.9174149632453918, Validation Loss: 0.5509585854145944\n",
      "Epoch 7, Training Loss: 0.9109825491905212, Validation Loss: 0.55044854541284\n",
      "Epoch 8, Training Loss: 0.9517418146133423, Validation Loss: 0.5504655188054606\n",
      "Epoch 9, Training Loss: 0.9082638621330261, Validation Loss: 0.5502392383498977\n",
      "Epoch 10, Training Loss: 0.9558299779891968, Validation Loss: 0.550381864360465\n",
      "Test Loss: 0.5502163393344912\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # train\n",
    "    for pair, label in zip(X_train, y_train):\n",
    "        features1, features2 = torch.tensor(pair[0]).float(), torch.tensor(pair[1]).float()  # 转换为 float 类型\n",
    "        \n",
    "        # calculate score for each group\n",
    "        score1 = model(features1)\n",
    "        score2 = model(features2)\n",
    "        \n",
    "        # The goal of RankNet is to make the difference between score1 and score2 reflect the label\n",
    "        target = torch.tensor([label], dtype=torch.float)  # transform to scalar\n",
    "        pred = score1 - score2  \n",
    "        \n",
    "        loss = criterion(pred, target)  \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for pair, label in zip(X_val, y_val):\n",
    "            features1, features2 = torch.tensor(pair[0]).float(), torch.tensor(pair[1]).float()  \n",
    "\n",
    "            score1 = model(features1)\n",
    "            score2 = model(features2)\n",
    "            target = torch.tensor([label], dtype=torch.float)  \n",
    "            pred = score1 - score2\n",
    "            \n",
    "            val_loss += criterion(pred, target).item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss/len(X_val)}')\n",
    "\n",
    "# test\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for pair, label in zip(X_test, y_test):\n",
    "        features1, features2 = torch.tensor(pair[0]).float(), torch.tensor(pair[1]).float()  \n",
    "        \n",
    "        score1 = model(features1)\n",
    "        score2 = model(features2)\n",
    "        target = torch.tensor([label], dtype=torch.float) \n",
    "        pred = score1 - score2\n",
    "        \n",
    "        test_loss += criterion(pred, target).item()\n",
    "\n",
    "print(f'Test Loss: {test_loss/len(X_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7305167905756789\n",
      "Precision: 0.7153496548337352\n",
      "Recall: 0.6850732408711021\n",
      "F1-Score: 0.6998841676982424\n",
      "Confusion Matrix: \n",
      "[[302986  91003]\n",
      " [105132 228698]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "# validate accuracy, recall, precision, confusion matrix\n",
    "model.eval()  \n",
    "\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for pair, label in zip(X_test, y_test):  \n",
    "        features1, features2 = torch.tensor(pair[0]).float(), torch.tensor(pair[1]).float()  \n",
    "        score1 = model(features1)\n",
    "        score2 = model(features2)\n",
    "        pred = score1 - score2\n",
    "\n",
    "        predicted_label = 1 if pred > 0 else 0\n",
    "        predicted_labels.append(predicted_label)\n",
    "        true_labels.append(label)\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(f\"Confusion Matrix: \\n{conf_matrix}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
